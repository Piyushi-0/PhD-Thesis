\chapter{Conclusion and Future Directions}\label{chap:conclude}
Our research started with the motivation to improve the statistical performance of the OT formulations by investigating the role of kernel-based regularizers. Through significant contributions in the form of rigorous theoretical results and a diverse set of experiments, we corroborate our thesis statement of improving the OT formulations using kernel methods.

In Chapter~\ref{chap:ch1}, we introduced a novel family of metrics by carefully integrating kernel-based regularization in the OT formulation. These metrics enjoy the best of both worlds by having geometrical properties like that of the Wasserstein metric and by exhibiting a dimension-free sample complexity like that of the kernel-based Maximum Mean Discrepancy metric. Furthermore, we discussed finite-sample-based estimators for these metrics whose statistical complexity is again free of dimensionality. To the best of our knowledge, such results are not known for other variants of OT. Leveraging the structure of the corresponding optimization problem, we propose solvers based on accelerated projected gradient descent, which matches the computational performance of the Sinkhorn solver. Our proposed approach leads to statistically efficient divergence in high-dimensions (Sec.~\ref{exp:tst}, Sec.~\ref{jumbot-mnist}), improves over the state-of-the-art baseline for prompt learning in few-shot classification (Sec.~\ref{exp:prompt-uot}) and our derived barycenter shows superior performance as an interpolating measure (Sec.~\ref{exp:bary}).

In Chapter~\ref{chap:ch2}, we proposed a framework to induce controlled structural sparsity in the OT plan. Using the smoothness and strong convexity of the optimization objective that results from squared-MMD-based regularization, we show an equivalence to a submodular maximization problem. We also devised efficient gradient-based greedy approximation algorithms to solve the resulting optimization efficiently, which showcased better optimization quality than the existing approaches of approximating the continuous version of the problem. Our results not only demonstrate more interpretable and efficient alignments (Sec.~\ref{gensparseexp}, Sec.~\ref{exp:word-alignment}) but also show a competitive performance on downstream applications (Sec.~\ref{colsparseexp}).

In Chapter~\ref{chap:ch3}, we contributed the first (to the best of our knowledge) statistically consistent OT formulation over conditional measures. By carefully integrating kernelized least-square-regression terms, we tackle the challenging problem of comparing conditionals when the empirical samples are provided from the joint measures and not those corresponding conditionals. We demonstrated modeling the conditional transport plan using parametric explicit as well as implicit models (facilitated by the MMD regularization terms that do not restrict the support of measures to overlap). The resulting conditional generator outperformed the baselines for the experiments on cell-population dynamics (Sec.~\ref{sec:simbio}), showing its efficacy as a conditional unpaired data generation, and also improved the downstream performance on the task of prompt learning for few-shot classification (Sec.~\ref{sec:prompt}).

The contributions presented in this thesis underscore the critical role of kernel-based regularization in overcoming the limitations of traditional OT formulations. These advancements not only enhance the practical utility of OT but also open new avenues for future research at the intersection of OT and kernel methods for comparing measures, ultimately contributing to the broader field of machine learning.
\paragraph{Future Directions.} We conclude by listing several interesting open problems and future directions that arose from our research.
\begin{enumerate}
\item{Possible extensions to Chapter~\ref{chap:ch1}:}
    \begin{enumerate}
        \item \textbf{More on metricity.} 
        \begin{enumerate}\label{metricityopen}
            \item It is an open problem to prove/disprove the triangle inequality (needed for metricity) for OT regularized with IPM raised to the power $q(>1)$, without which this gives a semi-metric (Sec.~\ref{sec:ipmq}). This formulation is particularly attractive with squared-MMD regularization due to the computational efficiency it brings (Sec. \ref{comput}) while maintaining the dimension-free sample complexity (Lemma \ref{lemma:sampcomp2}). Our ongoing attempts have included the construction of arguments based on the Gluing lemma used for such proofs in traditional OT formulations \citep{villanioldnew,cuturi13a,Piccoli2014GeneralizedWD}.\label{mmd2OTproof}
            \item Studying the metric properties of the divergence resulting from raising the cost function to the power $p(>1)$, analogous to $p$-Wasserstein, in the proposed formulation (Eq.~\ref{eqn:proposed}) is an open problem.
        \end{enumerate}
        \item \textbf{On closed-form solutions.} It is an open problem to investigate if our proposed metrics exhibit a closed-form solution. Such closed-form solutions have been studied for entropy-regularized OT with Gaussian measures and squared-Euclidean cost functions in \cite{JanatiMPC20}.
        \item \textbf{On computational aspects of a general IPM-regularized OT.} As mentioned in Remark~\ref{ipm-remark}, most of our theoretical results extend to the IPM-regularized OT formulation with a general IPM (that meterizes). Deriving scalable solvers to estimate these formulations is an open problem for IPMs other than MMD.
        \item \textbf{Other directions.} 
        \begin{enumerate}
            \item Exploring the applications of our generalization of Kantorovich-Rubinstein duality in Theorem~\ref{gen-kantdual} could be an interesting future work.
            \item Future directions of research would also include studying Gradient Flows \citep[Sec 2.5]{ChizatPSV18} for the space of measures endowed with the proposed metrics.
            \newline
            Our OT metrics with MMD(or, more generally, IPM)-regularization again belongs to the family of IPMs, and thus, the interpolating mixture measures \citep[Sec. (5.1)]{bottou2017geometrical} form its geodesics. It will be interesting to study the geodesics for the general OT divergences described in \ref{metricityopen}, i.e. the ones resulting from our OT formulation with squared-MMD regularization and our OT formulation with cost function raised to $p(>1)$.
            \item Besides the Unbalanced OT formulation we explore, one may also study other variants of the proposed kernel-regularized OT formulations like the Gromov-Wasserstein/Sliced-Wasserstein/Partial-OT variants of the Kantorovich OT \citep{peyre2019computational}. In particular, exploring the Gromov-Wasserstein-like divergence with the proposed kernel-based regularization would broaden the current scope of our applications to compare measures over incomparable spaces.
            \item It will also be an interesting future direction to study if the OT-based divergences improve the quality of kernel-mean-embeddings, which will strengthen the interplay between the two rich fields of OT and kernel methods.
        \end{enumerate}
    \end{enumerate}
\item{Possible extensions to Chapter~\ref{chap:ch2}:}
    \begin{enumerate}
        \item \textbf{More on structured-sparse OT plans.} Our proposed submodular framework to induce structured sparsity is designed for learning column-wise/row-wise sparsity patterns. As discussed in our work, such patterns help in budget-constrained mapping of `supply nodes' to `demand nodes'. As an extension, further structural patterns like block-wise sparsity can be induced. This would help in applications where a subset of source points needs to be mapped to a subset of target points, like comparing distributions over nodes of two graphs \citep{GOT} where prior knowledge decides the subset of nodes (sub-graphs) to be aligned.\newline
        Another interesting extension will be generalizing the proposed framework to learn low-rank OT plans like in \cite{lowrankuot}. 
        \item \textbf{More on computational performance.} For solving the problem of weakly submodular maximization under partition matroid constraints, we propose novel gradient-based greedy algorithms, an improvised version of the greedy algorithm for solving the problem under general matroid constraints \citep[Algorithm (1)]{pmlr-v80-chen18b}. We expect computational benefits if we instead devise a gradient-based greedy algorithm improving the existing algorithms designed specifically for partition matroid constraints \citep{partitionmalgo}. This may require extending the characterization of our set function for analyzing the `curvature ratio' and the `diminishing ratio' discussed in \cite{partitionmalgo}.
        \item \textbf{On statistical analysis.} In the current work, we focused on learning a structured solution to the OT problem. It will be interesting to investigate the statistical properties of the resulting OT divergence with such restrictions introduced to the search space.
    \end{enumerate}
\item{Possible extensions to Chapter~\ref{chap:ch3}:}
    \begin{enumerate}
        \item \textbf{On metricity.} While our proposed OT formulation between conditionals exhibits statistical consistency and performs well empirically, proof of the metric properties of this divergence is pending. We believe this proof could follow along the lines of the metricity proof of OT with squared-MMD regularization (open problem \ref{mmd2OTproof}).
        \item \textbf{More applications of our conditional generator.} It would be interesting to expand the applications of our conditional generator, learned through the COT formulation, and evaluate its performance against related approaches like \cite{Kim2022ConditionalWG}.
        \item \textbf{More on barycenter between conditionals.} Our proposed approach to generate the interpolating barycenters between conditionals ($\S$~\ref{sec:simbary}) is limited to the case when we have two conditionals. Formulating for the general conditional barycenter problem between any no. of measures could be a future line of work.
    \end{enumerate}
\end{enumerate}