\abstract{
The Optimal Transport (OT) formulation provides a metric over distributions by lifting the geometry over their support. This thesis focuses on the OT formulation proposed by Kantorovich, which seeks a probabilistic transport plan to optimally couple one distribution with another in the most cost-efficient manner. As comparing distributions is at the core of several machine learning (ML) algorithms, OT has found many applications in ML. While the original optimization is bottlenecked by the computational complexity of solving a linear program, regularizing the objective with the entropy of the transport plan benefits from scalable solvers based on the Sinkhorn algorithm. Such entropy-regularized variants have extended the application of OT to large-scale settings that commonly arise in ML. 

% However, a few key challenges still hinder the usage of OT in ML, particularly when dealing with high-dimensional data, when one needs a sparse-structured transport plan, or while comparing conditionals without having access to the samples from those conditioned measures. This thesis focuses on addressing these challenges by leveraging the theory of kernel methods. 

% In most ML applications, one does not have access to the true measures but only has access to the empirical distribution over finite $m$ samples.
This thesis begins with a crucial observation that although the geometry induced by OT-based Wasserstein metrics is rich, the sample complexity required to estimate these metrics suffers from the curse of dimensionality.
With $m$ as the no. of samples from the measures involved, the minimax estimation error of Wasserstein is known to be $O(m^{-1/d})$, which has an adversarial dependence on the underlying data dimensions. On the other hand, the kernel-based metric, Maximum Mean Discrepancy (MMD), has a dimension-free sample complexity, but the induced geometry is often independent of the kernel employed to capture the same. This leads us to explore if kernel-based regularization can help fix the sample complexity of OT while preserving its metric-related properties. Interestingly, our research results in a new family of metrics with geometry analogous to Wasserstein and a dimension-free sample complexity. We propose novel OT formulations over (potentially unnormalized) measures with kernel-based MMD regularizers to match the transport plan's marginals with the given source and target measures. Furthermore, our careful choice of the squared-MMD metric enables us to exploit the smoothness of the resulting objective to come up with scalable solvers. We derive corresponding theoretical guarantees and showcase improvements to downstream applications.

The first challenge we address is that of efficiently estimating OT in high dimensional settings when kernel-based regularization is employed to match the marginals of the transport plan to the given source and target measures. A key contribution of this work is showing that this regularized OT variant not only removes the curse of dimensionality issue in OT but also maintains the metric properties. 
Our proposed formulation introduces novel statistically efficient metrics belonging to the class of Integral Probability Metrics and obtains an estimation error of $O(m^{-1/2})$. 
% more properties
Moreover, we also derive the estimation error when the support of the transport plan is restricted to (a subset of) finite samples available from the source and target measures, as is the case in practical scenarios. We show that, under certain assumptions, the estimation error with this finite parametrization also comes out to be $O(m^{-1/2})$. To the best of our knowledge, such dimension-free error bounds are not known for other variants of regularized OT.
Further, leveraging the smoothness of the optimization objective induced by squared-MMD regularization, we demonstrate an accelerated projected gradient descent solver that matches the computational efficiency of the Sinkhorn solver popularly used for entropy-regularized OT.

Next, we use the proposed OT formulation with kernel-based regularization to alleviate the issue of non-sparsity in transport plans that result from entropy regularization in OT. While the entropy-regularized OT variants are computationally attractive, the resulting transport plans are dense, with each entry being strictly positive.  This, in turn, affects the interpretability of the alignments
obtained through the transport plan. We discuss how kernel-based regularization allows us to induce structured sparsity in the finitely parameterized transport plan.
% We first show that when one uses squared-MMD regularization to match the marginals involved in the OT formulation, the resulting objective becomes strongly convex and smooth. 
Leveraging the smoothness and strong convexity of our optimization objective, we recast the problem of finding a sparse solution to the optimal transport problem to that of a submodular maximization problem. We propose algorithmic solvers, derive approximation guarantees, and show that the duality gap for the problem obtained with the proposed approach is better than that of the existing approaches. We highlight that other popular variants of OT do not enjoy a smooth objective function and, hence, can not benefit from our proposed approach to this problem. This again shows the efficacy of the proposed kernel-based regularization in the OT formulation. 
% While greedy algorithms with constant-factor approximation ratios have been studied for maximizing a submodular function under matroid constraints, these algorithms may incur significant costs for repeated function calls. We improvise these algorithms to propose efficient gradient-based greedy algorithms with similar approximation guarantees. Finally, we show that the duality gap obtained with the proposed approach is better than the existing approaches.

Finally, we demonstrate this thesis idea to solve OT problems involving conditional measures, which are implicitly specified through joint samples. 
Given samples from two joint distributions, we consider the problem of OT between them when conditioned on a common variable. 
% This problem naturally appears when one needs to train an implicit conditional generative model using a Wasserstein-style loss. 
This setting is particularly difficult when the conditioned variable is continuous. The key challenge in estimating OT in this setting comes in enforcing the marginal-matching constraints involving the conditionals, as the samples provided are not from the conditionals but from the joints. Our formulation employs kernelized least-squares-based MMD-regularizers computed over the joint samples to implicitly match the transport planâ€™s marginals with the empirical conditionals.
We also discuss learning the transport plan with parametric models. As the MMD metric is meaningful even with measures having non-overlapping support, this facilitates us in employing implicit generative models to learn the transport plan. Under mild conditions, the error in estimating the divergence between the conditionals decays at $O(m^{-1/4})$ where $m$ is the no. of samples. To the best of our knowledge, ours is the first consistent OT formulation between empirical conditional measures.

We empirically validate the theoretical guarantees of our proposed formulations. We show improvements in diverse applications such as domain adaptation, two-sample hypothesis tests, prompt learning for few-shot classification, cell-population dynamics, aligning words in sentence pairs and learning mixture-of-experts models.

This thesis underscores the critical role of kernel-based regularization in various formulations of optimal transport. In particular, we demonstrate how the choice of such regularization results in dimension-free estimation error while maintaining the metric properties, helps efficiently induce sparsity structure on the transport plan and results in a consistent formulation for comparing conditional measures. We believe this research will open avenues for future studies on the interplay between OT and kernel methods.
}
